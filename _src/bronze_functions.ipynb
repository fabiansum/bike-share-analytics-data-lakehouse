{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "afc9392c-a412-4cbd-b574-9eea5b29414f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_account_name = \"bikesharedlake\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a07fecc9-a8e2-4569-902b-218a8b21a67a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import input_file_name\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "from pyspark.sql.dataframe import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ee46757-52e5-4bce-981b-1300d1edd868",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_files(layer, file_name, file_format, col_names=None, schema=None):\n",
    "    \"\"\"\n",
    "    Parameter layer is the medallion layer: landingzone, bronze, silver, gold\n",
    "    Parameter path is the relative folder structure that all files should be read from\n",
    "    Parameter file_format allows for selection of file formats like CSV or Parquet\n",
    "    Parameter column_names is a list of column names to use for the DataFrame\n",
    "    Parameter schema is a schema for the DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    # constructing full path\n",
    "    full_path = f\"/mnt/{storage_account_name}/{layer}/{file_name}.{file_format}\"\n",
    "    message = 'reading file from: ' + full_path\n",
    "    print(message)\n",
    "\n",
    "    try:\n",
    "        # reading parquet files to DataFrame\n",
    "        if col_names is not None and schema is None:\n",
    "            df = spark.read.format(file_format).option(\"header\", True).load(full_path).toDF(*col_names)\n",
    "        elif col_names is not None and schema is not None:\n",
    "            df = spark.read.format(file_format).option(\"header\", True).schema(schema).load(full_path).toDF(*col_names)\n",
    "        elif col_names is None and schema is not None:\n",
    "            df = spark.read.format(file_format).option(\"header\", True).schema(schema).load(full_path)\n",
    "        else:\n",
    "            df = spark.read.format(file_format).load(full_path)\n",
    "        message = \"file successfully read from:\" + full_path\n",
    "        print(message)\n",
    "\n",
    "    except:\n",
    "        # failure message\n",
    "        message = \"Failed to read file from:\" + full_path\n",
    "        print(message)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f545816-6fce-4f7e-89d5-e279fa601e41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def df_add_columns(df, add_timestamp=False, add_filename=False):\n",
    " \n",
    "    \"\"\"\n",
    "    This function adds bronze layer custom fields\n",
    "    Parameter add_timestamp determines whether timestamp should be added to dataframe\n",
    "    Parameter add_filename determines whether landing zone path and filename should be added to dataframe\n",
    "    \"\"\"\n",
    "    #add timestamp column\n",
    "    if add_timestamp:\n",
    "        df = df.withColumn(\n",
    "            \"ingestion_date\",\n",
    "            current_timestamp()\n",
    "        )\n",
    " \n",
    "    #add full path and filename column\n",
    "    if add_filename:\n",
    "        df = df.withColumn(\n",
    "            \"bronze_filename\",\n",
    "            input_file_name()\n",
    "        )\n",
    " \n",
    "    return df\n",
    " \n",
    "#extending DataFrame class with add_columns function \n",
    "DataFrame.add_columns = df_add_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "baeb0f43-0a8d-419d-a03f-249ee0f33fbc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_parquet_table(df, layer, table_name):\n",
    " \n",
    "    \"\"\"\n",
    "    This function will write dataframe df to storage account as a parquet files\n",
    "    Parameter table_name defines parquet file name in bronze layer \n",
    "    \"\"\"\n",
    " \n",
    "    #defining full path to in storage account\n",
    "    full_path = f\"/mnt/{storage_account_name}/{layer}/{table_name}\"    \n",
    "    message = 'writing bronze file: ' + full_path\n",
    "    print(message)\n",
    "    \n",
    "    try:\n",
    "        #writing table folder with parquet files in overwrite mode for SCD1\n",
    "        df.write.mode('overwrite').parquet(full_path)\n",
    "        message = 'Table successfully written: ' + full_path\n",
    "        print(message)\n",
    " \n",
    "    except:\n",
    "        #error message\n",
    "        message = 'Failed to write table: ' + full_path\n",
    "        print(message)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_functions",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
